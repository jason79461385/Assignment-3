import sys
import os
import datetime
import warnings
import re
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="langchain_google_genai")
warnings.filterwarnings("ignore", message=".*Convert_system_message_to_human.*")
warnings.filterwarnings("ignore", message=".*API key must be provided when using hosted LangSmith API.*")
import time
from termcolor import colored
from graph_agent import run_graph_agent, run_legacy_agent
from config import get_llm
from langchain_core.prompts import ChatPromptTemplate

TEST_MODE = "LEGACY" # Options: "GRAPH" or "LEGACY"

class DualLogger:
    def __init__(self, filename="evaluation_log.txt"):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")
        self.ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')

    def write(self, message):
        self.terminal.write(message) 
        clean_message = self.ansi_escape.sub('', message)
        self.log.write(clean_message)      
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()

def grade_answer_with_llm(question, agent_answer, expected_facts, forbidden_facts):
    llm = get_llm(temperature=0) 
    
    prompt = ChatPromptTemplate.from_template("""
    You are a strict grading assistant. Check if the AGENT_ANSWER meets the following criteria based on the QUESTION.
    
    QUESTION: {question}
    AGENT_ANSWER: {agent_answer}
    
    CRITERIA 1 (Must Include): The answer MUST semantically contain these facts: {expected_facts}.
    CRITERIA 2 (Forbidden): The answer MUST NOT contain information about: {forbidden_facts}.
    
    Logic:
    - If the agent says "391 billion" and expected is "391,000 million", that is PASS.
    - If the agent answers in a different language but the numbers/facts are correct, that is PASS.
    - If the agent hallucinates or mentions forbidden topics (e.g., mentioning Tesla when asked about Apple), that is FAIL.
    
    OUTPUT ONLY ONE WORD: "PASS" or "FAIL".
    """)
    
    chain = prompt | llm
    result = chain.invoke({
        "question": question, 
        "agent_answer": agent_answer,
        "expected_facts": str(expected_facts),
        "forbidden_facts": str(forbidden_facts)
    })
    
    return result.content.strip().upper()

TEST_CASES = [
    {
        "name": "Test A: Apple Revenue",
        "question": "Apple 2024 å¹´çš„ç¸½ç‡Ÿæ”¶ (Total net sales) æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["391", "billion", "3,910", "å„„"],
        "forbidden": ["Tesla"]
    },
    {
        "name": "Test B: Tesla R&D",
        "question": "Tesla 2024 å¹´çš„ç ”ç™¼è²»ç”¨ (R&D expenses) æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["4.77", "47.7", "billion"],
        "forbidden": ["Apple"]
    },
    
    {
        "name": "Test C: R&D Comparison",
        "question": "æ¯”è¼ƒ Apple å’Œ Tesla 2024 å¹´çš„ç ”ç™¼æ”¯å‡ºï¼Œèª°æ¯”è¼ƒé«˜ï¼Ÿ",
        "must_contain": ["Apple", "Tesla", "31", "4.7", "higher"],
        "forbidden": []
    },

    {
        "name": "Test D: Apple Services Cost",
        "question": "Apple 2024 å¹´çš„ã€Œæœå‹™æˆæœ¬ (Cost of sales - Services)ã€æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["25", "billion", "25,119"],
        "forbidden": []
    },
    {
        "name": "Test E: Tesla Energy Revenue",
        "question": "Tesla 2024 å¹´çš„ã€Œèƒ½æºç™¼é›»èˆ‡å„²å­˜ (Energy generation and storage)ã€ç‡Ÿæ”¶æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["23.7", "billion", "23,767"],
        "forbidden": []
    },

    {
        "name": "Test F: Semantic Trap (Profitability)",
        "question": "é€™å…©å®¶å…¬å¸åœ¨ 2024 å¹´èª°æ¯”è¼ƒæœƒè³ºéŒ¢ï¼Ÿ(æ¯”è¼ƒ Net Income)",
        "must_contain": ["Apple", "Tesla", "93", "7", "billion"],
        "forbidden": []
    },

    {
        "name": "Test G: Unknown Info",
        "question": "Apple è¨ˆç•«åœ¨ 2025 å¹´ç™¼å¸ƒçš„ iPhone 17 é è¨ˆå”®åƒ¹æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["unknown", "provide", "mention", "does not", "ç„¡æ³•", "æœªæåŠ"],
        "forbidden": ["1000", "999", "1200"] 
    },
    {
        "name": "Test A1 [Eng]: Apple Revenue",
        "question": "What was Apple's Total Net Sales for the fiscal year 2024?",
        "must_contain": ["391", "billion", "391,035"], 
        "forbidden": ["Tesla"]
    },
    {
        "name": "Test A2 [Eng]: Tesla Automotive Revenue",
        "question": "What is the specific revenue figure for 'Automotive sales' for Tesla in 2024?",
        "must_contain": ["78", "billion", "78,512"], 
        "forbidden": ["Apple"]
    },

    {
        "name": "Test B1 [Mixed]: Apple R&D",
        "question": "Apple 2024 å¹´çš„ç ”ç™¼è²»ç”¨ (Research and development expenses) æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["31", "billion", "31,370"], 
        "forbidden": ["Tesla"]
    },
    {
        "name": "Test B2 [Mixed]: Tesla CapEx",
        "question": "Tesla åœ¨ 2024 å¹´çš„è³‡æœ¬æ”¯å‡º (Capital Expenditures) æ˜¯å¤šå°‘ï¼Ÿ",
        "must_contain": ["11", "billion", "11,153"], 
        "forbidden": ["Apple"]
    },
    {
        "name": "Test C1 [Eng]: R&D Comparison",
        "question": "Compare the Research and Development (R&D) expenses of Apple and Tesla in 2024. Who spent more?",
        "must_contain": ["Apple", "Tesla", "31", "4.7", "Apple spent more", "higher"],
        "forbidden": [] 
    },
    {
        "name": "Test C2 [Eng]: Gross Margin Analysis",
        "question": "Which company had a higher Total Gross Margin percentage in 2024, Apple or Tesla? Please provide the approximate percentages.",
        "must_contain": ["Apple", "Tesla", "46", "18", "Apple"],
        "forbidden": []
    },

    {
        "name": "Test D1 [Eng]: Apple Services Cost",
        "question": "According to the Consolidated Statements of Operations, what was Apple's 'Cost of sales' specifically for 'Services' in 2024?",
        "must_contain": ["25", "billion", "25,119"],
        "forbidden": []
    },
    
    {
        "name": "Test E1 [Mixed]: 2025 Projection (Trap)",
        "question": "è²¡å ±ä¸­æœ‰æåˆ° Apple 2025 å¹´é è¨ˆçš„ iPhone éŠ·é‡ç›®æ¨™å—ï¼Ÿ",
        "must_contain": ["no", "not mentioned", "does not provide", "æ²’æœ‰æåˆ°", "æœªçŸ¥"], 
        "forbidden": ["100 million", "200 million", "increase"] # ç¦æ­¢çæ°æ•¸å­—
    },
    
    {
        "name": "Test F1 [Eng]: CEO Identity",
        "question": "Who signed the 10-K report as the Chief Executive Officer for Tesla?",
        "must_contain": ["Elon Musk"],
        "forbidden": ["Tim Cook"]
    }
]

def run_evaluation():
    score = 0
    total = len(TEST_CASES)

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n==================================================")
    print(f"ğŸ“„ ASSIGNMENT 3 EVALUATION REPORT")
    print(f"â° Time: {timestamp}")
    print(f"ğŸ¤– Agent Mode: {TEST_MODE}")
    print(f"==================================================\n")

    print(colored("ğŸš€ STARTING AI-POWERED EVALUATION...", "cyan", attrs=["bold"]))
    print(f"==================================================\n")


    for test in TEST_CASES:
        print(f"Running: {test['name']}...")
        start_time = time.time()
        
        try:
            if TEST_MODE == "GRAPH":
                answer = run_graph_agent(test["question"])
            else:
                answer = run_legacy_agent(test["question"])
            
            result = grade_answer_with_llm(
                test["question"], 
                answer, 
                test["must_contain"], 
                test["forbidden"]
            )
            
            elapsed = time.time() - start_time
            print(f"A: {answer.strip()}")
            if "PASS" in result:
                score += 1
                print(colored(f"âœ… PASS ({elapsed:.2f}s)", "green"))
            else:
                print(colored(f"âŒ FAIL ({elapsed:.2f}s)", "red"))
                print(f"   Agent Answer: {answer}")
                print(f"   Judge Verdict: {result}")

        except Exception as e:
            print(colored(f"âŒ CRASH: {e}", "red"))
        print("-" * 50)

    print(colored(f"\nğŸ“Š FINAL SCORE: {score}/{total}", "magenta", attrs=["bold"]))

if __name__ == "__main__":
    log_filename = f"evaluation_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt"
    sys.stdout = DualLogger(log_filename)
    run_evaluation()
    sys.stdout = sys.stdout.terminal
    print(f"\n[System] Log saved to {log_filename}")